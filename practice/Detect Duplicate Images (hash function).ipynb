{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚合相似图片\n",
    "\n",
    "QQ 缓存的群表情实在是太多了...而且图片的命名简直就是乱码，每次手工整理都很费劲，虽然顺便收表情包很开心xd\n",
    "\n",
    "最近在重新学 Python ，就想用来实践一下。之前数字内容安全还是什么课有用 Python 写过感知哈希（pHash），按照流程一步步实现来的，当时只验证了 pHash 可以抵抗轻微的图片修改，属于鲁棒哈希。\n",
    "\n",
    "于是就想着应该可以用来检测重复图片，找了个支持图像哈希的模块 [ImageHash](https://pypi.org/project/ImageHash/) ，其中还有其他感知哈希（dHash、aHash、wHash），全部都拿来尝试了一下。\n",
    "\n",
    "根据哈希值差值，将图片进行分组，实现起来不难，但是有两个缺点：\n",
    "- 哈希算法的选择\n",
    "- 差值的选择\n",
    "\n",
    "简直就是“凭感觉”的选择，效果确实有，可以聚合一些相似图片，但是也有完全不相干的图片被聚合的情况...\n",
    "\n",
    "之后打算拿 [Image Deduplicator](https://idealo.github.io/imagededup/) 试一试，由于 notebook 运行在 Python3.8.5 的虚拟环境中，不支持 imagededup，所以就先实现一个简单的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import imagehash\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image path into list\n",
    "def read_files(path):\n",
    "    images = []\n",
    "    for file in os.listdir(path):\n",
    "        filename = file.lower()\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\"jpeg\") or \\\n",
    "            filename.endswith(\".png\") or filename.endswith(\"gif\"):\n",
    "            filepath = path + filename\n",
    "            images.append(filepath)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open image and calculate hash \n",
    "def cal_hash(files, convert_str=True, sort=True, hashfunc=imagehash.dhash):\n",
    "    unhandled = []\n",
    "    hashes = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            image = Image.open(f)\n",
    "            h = hashfunc(image)\n",
    "            if convert_str:\n",
    "                h = str(h)    # convert to hex string\n",
    "            hashes.append((str(h), f))\n",
    "        except Exception as e:\n",
    "            print('Problem:', e, 'with', f)\n",
    "            unhandled.append(f)\n",
    "            continue\n",
    "    \n",
    "    if sort:    # sort by hex value\n",
    "        hashes.sort(key=lambda x:x[0])\n",
    "    \n",
    "    return hashes, unhandled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide images into groups with little hash difference\n",
    "def divide_groups(hashes, threshold=1):\n",
    "    # divide into groups by diff\n",
    "    tmp = []\n",
    "    groups = []\n",
    "    for i,v in enumerate(hashes):\n",
    "        if i+1 > len(hashes)-1:\n",
    "            break\n",
    "        \n",
    "        h1, f1 = v\n",
    "        h2, f2 = hashes[i+1]\n",
    "        diff = imagehash.hex_to_hash(h1) - imagehash.hex_to_hash(h2)\n",
    "        if abs(diff) < threshold:\n",
    "            tmp.append(v)\n",
    "        else:\n",
    "            if tmp:\n",
    "                tmp.append(v)\n",
    "                groups.append(tmp)\n",
    "                tmp = []\n",
    "    \n",
    "    # not in group\n",
    "    flatten = list(itertools.chain(*groups))\n",
    "    res_hashes = [(h,f) for h,f in hashes if (h,f) not in flatten]\n",
    "    \n",
    "    print(\"Origin:\", len(hashes), \"Total:\", len(flatten)+len(res_hashes),\n",
    "         \"Groups:\", len(groups), \"Grouped:\", len(flatten), \"Remain:\", len(res_hashes))\n",
    "    \n",
    "    return groups, res_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move and rename by groups\n",
    "def move_files(groups, store_path, filenum=0):\n",
    "    for lst in groups:\n",
    "        size = 0\n",
    "        for h,f in lst:\n",
    "            tmp = os.path.getsize(f)\n",
    "            if tmp > size:\n",
    "                saved = f\n",
    "                size = tmp\n",
    "        \n",
    "        # lagest file without \"_\" in filename\n",
    "        ext = \".\" + saved.split(\".\")[-1]\n",
    "        shutil.move(saved, store_path+str(filenum).zfill(5)+ext)\n",
    "        \n",
    "        # other duplicate files, with \"_N\" in filename\n",
    "        i = 1\n",
    "        for h,f in lst:\n",
    "            if f == saved:\n",
    "                continue\n",
    "            ext = \".\" + f.split(\".\")[-1]\n",
    "            shutil.move(f, store_path+str(filenum).zfill(5)+\"_\"+str(i)+ext)\n",
    "            i += 1\n",
    "        \n",
    "        # add filenum\n",
    "        filenum += 1\n",
    "    \n",
    "    # return for next function call\n",
    "    return filenum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_qq --> QQ 号\n",
    "path = \"D:/Program Data/QQ/my_qq/Image/Group/\"\n",
    "store = \"D:/Program Data/QQ/my_qq/Image/Group/sorted/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image path\n",
    "files = read_files(path)\n",
    "\n",
    "# calculate hash\n",
    "hashes, unhandled = cal_hash(files,hashfunc=imagehash.dhash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error opening image\n",
    "unhandled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4726"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total images\n",
    "len(hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin: 4726 Total: 4726 Groups: 216 Grouped: 507 Remain: 4219\n",
      "Origin: 4219 Total: 4219 Groups: 26 Grouped: 60 Remain: 4159\n",
      "Origin: 4159 Total: 4159 Groups: 47 Grouped: 97 Remain: 4062\n",
      "Origin: 4062 Total: 4062 Groups: 129 Grouped: 263 Remain: 3799\n"
     ]
    }
   ],
   "source": [
    "newhashes = hashes.copy()  # group on copy\n",
    "threshold = 20     # compare threshold\n",
    "filenum = 0        # filename\n",
    "for i in range(9, threshold, 3):\n",
    "    groups, newhashes = divide_groups(newhashes, i)\n",
    "    filenum = move_files(groups, store, filenum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚合之后的效果看起来还不错\n",
    "\n",
    "![](img/dhash.png)\n",
    "\n",
    "但是随着阈值的增加，不相干的图片也被分在一个组里了... 所以只是有点帮助的代码而已(´･ᴗ･`)\n",
    "\n",
    "![](img/errgroup.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename remaining files\n",
    "def rename_files(hashes, path, filenum):\n",
    "    for h,f in hashes:\n",
    "        ext = \".\" + f.split(\".\")[-1]\n",
    "        shutil.move(f, path+str(filenum).zfill(5)+ext)\n",
    "        filenum += 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_files(newhashes, store, filenum)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "编辑元数据",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
